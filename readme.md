<div align="center">
  <a href="https://docs.ros.org/en/humble/index.html">
    <picture>
      <img alt="ROS2" src="https://github.com/Robotic-System-Lab/Lintic-360/blob/main/assets/ROS-2_logo.png" height="128">
    </picture>
  </a>
  <h1 style="margin-top:12px;">Lictic 360</h1>

<a href="https://wiki.ros.org/noetic">
	<img alt="ROS2" src="https://img.shields.io/badge/ROS1-Noetic-88b55a?style=for-the-badge&logo=ros&logoColor=ffffff">
</a>
<a href="https://docs.ros.org/en/humble/index.html">
	<img alt="ROS2" src="https://img.shields.io/badge/ROS2-Humble-336c9a?style=for-the-badge&logo=ros&logoColor=ffffff">
</a>
<a href="https://github.com/dusty-nv/jetson-inference">
	<img alt="YOLO" src="https://img.shields.io/badge/ResNet-18-50b730?style=for-the-badge&logo=nvidia&logoColor=50b730">
</a>
<a href="https://github.com/ultralytics/ultralytics">
	<img alt="YOLO" src="https://img.shields.io/badge/YOLO-V11-0f81c2?style=for-the-badge&logo=yolo&logoColor=8cebf7">
</a>
</div>
(LiDAR and Camera Semantic 360) - A ROS2 project that aims to develop a semantic mapping system using a 360-degree camera array, integrated with 2D LiDAR. The system will convert panoramic images captured by the cameras into 2D maps that generated by using GMapping. The semantic map classify various elements of the environment. By using semantic mapping techniques, autonomous vehicles or robots will be better able to understand and navigate their surroundings by providing richer context about the objects and areas around them.

## Developer Notes
This repository focused on applying semantic mapping for Husky Clearpath A200 while also working with Jetson AGX Orin. However, it's still possible to use this repo as reference to working on Semantic Mapping. The difference would only appear on how to implement the structure of the project.

## Project Structure
```
.
├──src
│  ├──lictic
│  └──utils
│     ├── merger
│     ├── visual
│     ├── gmapper
│     ├── yolosed
│     ├── velodyne
│     └── ros_deep_learning
│
└──support
   ├── openslam_gmapping
   └── relay
```

This project placed the required packages together inside `utils` folder. The lictic inside `./src` meant to only contain launch files and their configuration, while the remaining packages are described below:
1. `merger`: Transforms tf_tree from /odom and /scan to match each other and limit the LiDAR range
2. `visual`: Visualize mapping results into PNG file inside `~/lictic/captured_map/[timestamp]`
3. `gmapper`: LiDAR mapper node
4. `yolosed`: Packages for image processing. (Image detection was deprecated and no longer used)
5. `velodyne`: LiDAR launcher (only match VLP 16, 32C, 128)
6. `ros_deep_learning`: Camera launcher for Jetson Devices

## How To Build
### Prerequisite
- ROS2 Humble at Jetson AGX Orin
- ROS1 Noetic at Husky Clearpath A200
- ROS Bridge for Humble [(GitHub: TommyChangUMD)](https://github.com/TommyChangUMD/ros-humble-ros1-bridge-builder)
- LiDAR for ROS, e.g. [(Velodyne)](https://github.com/ros-drivers/velodyne.git) or another 2D LiDAR
- YOLO by [Ultralytics](https://github.com/ultralytics/ultralytics.git)
- CMake9 or higher
- PythonQT 6

### Installation
Assuming we already installed the prerequisites and understood the basics of ROS, we can continue on these steps. 
1. Clone this [Repository](https://github.com/Robotic-System-Lab/Lintic-360.git) into your ROS2 workspace.
```bash
mkdir -p ~/lintic_ws && cd ~/lintic_ws
git clone https://github.com/Robotic-System-Lab/Lintic-360.git .
```
2. We're using 2 main devices (Husky and Jetson), so we need to create a connection between them. In this case, I already set-up the connection by using a Ethernet Cable. We need to do some basic preparations in this case.
```bash
# Husky Clearpath A200 at `~/.bashrc`
export ROS_MASTER_URI=http://192.168.131.1:11311
export ROS_IP=192.168.131.1

# Jetson AGX Orin at `~/.bashrc`
export ROS_MASTER_URI=http://192.168.131.1:11311
```
3. Using raw published topics will not work since Husky has a complex network settings. Create this file and run `rosparam load bridge.yaml` to setup for `/odom` topic bridge.
```yaml
# Husky Clearpath A200 at `bridge.yaml`
topics:
  -
    topic: /odom
    type: nav_msgs/msg/Odometry
    queue_size: 10
```
4. In the Husky, we will need to create a package to run `relay` node. This node will run in ROS1. Make sure to modify the CMakeList.txt and put the `executable` and `target_link_libraries`.
```bash
# ROS1
mkdir -p ~/relay_ws/src && cd ~/relay_ws/src
catkin_create_pkg relay roscpp nav_msgs tf2_ros geometry_msgs
```
5. Run the node and the `rostopic list` should include new topic called `/odom` beside the `/odometry/odometry_filtered`
```bash
# ROS1
rosrun relay odom
```
6. Since we're using two different devices, we need to register the Husky as a host in the Jetson. Run sudo nano `/etc/hosts` and add this line (sometimes, we also need to restart the Jetson to apply this change)
> Note that your Jetson AGX Orin should use ip in range 192.168.131.101-192.168.131.110
```bash
# Husky's IP should be like this according to the Official Documentation 
192.168.131.1 husky
```
7. The setups are done, install `nlohmann` to support GMapper process
```bash
sudo apt-get -y install nlohmann-json3-dev
```
8. Next, we can navigate into `support/openslam_gmapping/` and we will install customized library that was originally made by [siddarth09](https://github.com/siddarth09/ros2_gmapping)
```bash
# ROS2
cd ~/lintic_ws/support/
mkdir build
cmake ..
sudo make install
```
9. Installing Ultralytics and PyTorch
> Note that the project was built to run in Jetson AGX Orin, the version of PyTorch is need to match JetPack 6.2. See this image docker for details: [ultralytics/ultralytics:latest-jetson-jetpack6](https://hub.docker.com/r/ultralytics/ultralytics/tags?name=jetpack6)
```bash
pip install numpy==1.26.4 https://github.com/ultralytics/assets/releases/download/v0.0.0/onnxruntime_gpu-1.20.0-cp310-cp310-linux_aarch64.whl https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-2.5.0a0+872d972e41.nv24.08-cp310-cp310-linux_aarch64.whl https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.20.0a0+afc54f7-cp310-cp310-linux_aarch64.whl ultralytics
```
> If u're going to run this on an AMD64 device, just run this instead:
```bash
pip install ultralytics
```
10. Build the whole packages
> The `ros_deep_learning` package was meant to launch camera for Jetson. If u're meant to run this project only with simulator, create a `COLCON IGNORE` file inside of the packages.
```bash
# ROS2
cd ~/lintic_ws/
colcon build
source install/setup.bash
```

## How To Run
1. You can change some parameters used to launch the full project in `./src/lictic/config/[media].yaml`
```yaml
# Segmentation configuration
segmentation:
  ros__parameters:
    segmentation_model: "yolo11m-seg.pt" # Segmentation model you want to use
    cam_center: 30  # (int) The center point of camera to match LiDAR's yaw
    cam_count: 6    # (int) Number of cameras used for 360 degrees array camera
    view_p: 0.35    # ( float ) Percentage of valid camera vertical FoV, used as threshold to match 2D LiDAR detection area
    view_h: 0.20    # ( float ) Percentage of valid camera vertical FoV height, used to match 2D LiDAR detection area
    fov_h: 0.001    # ( float ) Horizontal FoV of the camera (referring to the specification)
# LiDAR configuration
lidar_limiter:
  ros__parameters:
    maxrange: 5     # (int) LiDAR laser scan range limiter
```
2. Finally, we can run the launch file:
```bash
# Using Gazebo Simulator
ros2 launch lictic sim.launch.py

# Using Husky Robot
ros2 launch lictic husky.launch.py
```