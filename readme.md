# Lictic 360
(LiDAR and Camera Semantic 360) - This project aims to develop a semantic mapping system using a 360-degree camera array, integrated with 2D LiDAR. The system will convert panoramic images captured by the cameras into 2D maps that generated by using GMapping. The semantic map classify various elements of the environment. By using semantic mapping techniques, autonomous vehicles or robots will be better able to understand and navigate their surroundings by providing richer context about the objects and areas around them.

## Developer Notes
This repository focused on applying semantic mapping for Husky Clearpath A200 while also working with Jetson AGX Orin. However, it's still possible to use this repo as reference to working on Semantic Mapping. The difference would only appear on how to implement the structure of the project.

## Project Structure
```
.
├──src
│  ├── gmapper
│  ├── merger
│  ├── ros_deep_learning
│  ├── segnet
│  ├── yolosed
│  └── visual
│
└──support
   ├── openslam_gmapping
   └── relay
```

## How To Build
### Prerequisite
- ROS2 Humble at Jetson Orin
- ROS1 Noetic at Husky Clearpath A200
- ROS Bridge for Humble [(GitHub: TommyChangUMD)](https://github.com/TommyChangUMD/ros-humble-ros1-bridge-builder)
- YOLO by Ultralytics or Jetson Inference
- Velodyne LiDAR [(yavuzertugrul)](https://yavuzertugrul.com/Lidar_part_1#velodyne-vlp-16-lidar-ros2-humble-test-on-nvidia-jetson-orin-nx-8-gb-jetpack) or another 2D LiDAR
- CMake9 or higher
### Installation
Assuming we already installed the prerequisites and understood the basics of ROS, we can continue on these steps. 
1. Clone this [Repository](https://github.com/Robotic-System-Lab/Lintic-360.git) into your ROS2 workspace.
```bash
mkdir -p ~/lintic_ws && cd ~/lintic_ws
git clone https://github.com/Robotic-System-Lab/Lintic-360.git .
```
2. We're using 2 main devices (Husky and Jetson), so we need to create a connection between them. In this case, I already set-up the connection by using a Ethernet Cable. We need to do some basic preparations in this case.
```bash
# Husky Clearpath A200 at `~/.bashrc`
export ROS_MASTER_URI=http://192.168.131.1:11311
export ROS_IP=192.168.131.1

# Jetson Orin at `~/.bashrc`
export ROS_MASTER_URI=http://192.168.1.100:11311
export ROS_IP=192.168.1.103
```
3. Using raw published topics will not work since Husky has a complex network settings. Create this file and run `rosparam load bridge.yaml` to setup for `/odom` topic bridge.
```yaml
# Husky Clearpath A200 at `bridge.yaml`
topics:
  -
    topic: /odom
    type: nav_msgs/msg/Odometry
    queue_size: 10
```
4. In the Husky, we will need to create a package to run `relay` node. This node will run in ROS1. Make sure to modify the CMakeList.txt and put the `executable` and `target_link_libraries`.
```bash
# ROS1
mkdir -p ~/relay_ws/src && cd ~/relay_ws/src
catkin_create_pkg relay roscpp nav_msgs tf2_ros geometry_msgs
```
5. Run the node and the `rostopic list` should include new topic called `/odom` beside the `/odometry/odometry_filtered`
```bash
# ROS1
rosrun relay odom
```
6. Since we're using two different devices, we need to register the Husky as a host in the Jetson. Run sudo nano `/etc/hosts` and add this line (sometimes, we also need to restart the Jetson to apply this change)
```bash
# Husky's IP should be like this according to the Official Documentation 
192.168.131.1 husky
```
7. The setups are done, we can navigate into `support/openslam_gmapping/` and we will install customized library that was originally made by [siddarth09](https://github.com/siddarth09/ros2_gmapping)
```bash
cd ~/lintic_ws/support/
mkdir build
cmake ..
sudo make install
```
8. Build the whole packages
```bash
# The `ros_deep_learning` package was meant to launch camera for Jetson. If u're not using it, create a `COLCON IGNORE` file inside of the packages.

# Since we will only use one object detection for a single run, you can choose to build `segnet` or `yolosed`. (The segnet will run with Jetson Inference while yolosed run with YOLO by Ultralytics)

cd ~/lintic_ws/
colcon build
source install/setup.bash
```

## How To Run
After all those setups, we can run those nodes and launch files:
> Ignore the ros_deep_learning if u're not using it
```bash
ros2 launch ros_deep_learning video_source.ros2v2.launch
```
> You can change this if you're using another version or brand of LiDAR sensor
```bash
ros2 launch velodyne velodyne-all-nodes-VLP32C-launch.py
```
> Initiate the ROS Bridge
```bash
ros2 run ros1_bridge parameter_bridge
```
> Run the broadcaster for /odom
```bash
ros2 run merger odom
```
> Run the main GMapping process
```bash
ros2 run gmapper semap
```
> Run `segnet` if u want to use Jetson Inference or `yolosed` if u want to use YOLO instead
```bash
ros2 run segnet denset
ros2 run yolosed seg
```

## Common Issues
1. `nlohmann` related error encountered frequently, make sure to check every single error occured on ur machine. Run this if you found one.
```bash
sudo apt-get -y install nlohmann-json3-dev
```